import { credentialVaultClient } from "@dynatrace-sdk/client-classic-environment-v2";
import { result } from "@dynatrace-sdk/automation-utils";

/**
 * Part 3: HEAVY runs + partial-rerun logic (no policy filter)
 * - GET-only
 * - Pulls last N runs per PG with includeObjectDetails=true
 * - Tracks latest failure per object with NO later object-level success (within N runs), per RunType
 * Output consumed by Part 4 via result("cohesity_backup_failures_p3")
 */
export default async function () {
  const PART2 = "cohesity_backup_failures_p2";
  const p2 = await result(PART2);
  if (!p2?.baseUrl || !Array.isArray(p2.pgIndex)) {
    throw new Error(`Part 3: missing Part 2 output. Check step name: ${PART2}`);
  }

  const baseUrl = p2.baseUrl;
  const numRuns = Number(p2.numRuns ?? 20);
  const pgIndex = p2.pgIndex;

  // Vault auth
  const vaultId = "credentials_vault-312312";
  const d = await credentialVaultClient.getCredentialsDetails({ id: vaultId });
  const apiKey = (d?.token || d?.password || "").trim();
  if (!apiKey) throw new Error("No Helios API key available (empty token/password).");

  const commonHeaders = { accept: "application/json", apiKey };
  const sleep = (ms) => new Promise((r) => setTimeout(r, ms));

  async function getJsonWithRetry(url, headers, retries = 2) {
    for (let i = 0; i <= retries; i++) {
      const resp = await fetch(url, { method: "GET", headers });
      if (resp.ok) return resp.json();

      const st = resp.status;
      if ((st === 429 || st >= 500) && i < retries) {
        await sleep(600 * (i + 1));
        continue;
      }

      let txt = "";
      try { txt = await resp.text(); } catch (_) {}
      throw new Error(`GET ${url} -> HTTP ${st} ${txt}`);
    }
    return null;
  }

  function cleanMsg(s) {
    if (!s) return "";
    return String(s).replace(/[\r\n]+/g, " ").replace(/\|/g, " ").replace(/"/g, "'").trim();
  }

  function fmtETFromUsecs(usecs) {
    if (!usecs) return "";
    const ms = Math.floor(Number(usecs) / 1000);
    const dt = new Date(ms);
    try {
      return new Intl.DateTimeFormat("en-US", {
        timeZone: "America/New_York",
        year: "numeric",
        month: "2-digit",
        day: "2-digit",
        hour: "2-digit",
        minute: "2-digit",
        hour12: false,
      }).format(dt);
    } catch {
      return dt.toISOString();
    }
  }

  function getObjKey(ob) {
    const o = ob?.object;
    if (!o) return "";
    if (o.id) return String(o.id);
    const sid = o.sourceId ? String(o.sourceId) : "";
    return `${o.environment || ""}|${o.objectType || ""}|${o.name || ""}|${sid}`;
  }

  function hasFailedAttempts(ob) {
    const fa = ob?.localSnapshotInfo?.failedAttempts;
    return Array.isArray(fa) && fa.length > 0;
  }

  function isObjectSuccess(ob) {
    // Object present + no failedAttempts => success that clears older failures
    return ob?.localSnapshotInfo && !hasFailedAttempts(ob);
  }

  function combineFailedAttempts(attempts) {
    if (!Array.isArray(attempts) || attempts.length === 0) return "";
    return attempts.map((a) => cleanMsg(a?.message)).filter(Boolean).join(" | ");
  }

  async function mapLimit(items, limit, fn) {
    const out = [];
    let i = 0;
    const workers = Array.from({ length: Math.min(limit, items.length) }, async () => {
      while (true) {
        const idx = i++;
        if (idx >= items.length) break;
        const vals = await fn(items[idx], idx);
        if (Array.isArray(vals) && vals.length) out.push(...vals);
      }
    });
    await Promise.all(workers);
    return out;
  }

  const CONCURRENCY = 3; // heavy calls; keep conservative
  const pgErrors = [];

  async function processPg(pg) {
    const headers = { ...commonHeaders, Accept: "application/json", accessClusterId: String(pg.clusterId) };
    const url =
      `${baseUrl}/v2/data-protect/protection-groups/${encodeURIComponent(String(pg.pgId))}` +
      `/runs?numRuns=${numRuns}&excludeNonRestorableRuns=false&includeObjectDetails=true`;

    let jsonRuns;
    try {
      jsonRuns = await getJsonWithRetry(url, headers, 2);
    } catch (e) {
      pgErrors.push({ cluster: pg.clusterName, pg: pg.pgName, error: String(e?.message || e) });
      return [];
    }

    const runs = Array.isArray(jsonRuns?.runs) ? jsonRuns.runs : [];
    if (!runs.length) return [];

    const runTypes = Array.from(
      new Set(runs.map((r) => r?.localBackupInfo?.[0]?.runType).filter(Boolean).map(String))
    );

    const failuresOut = [];

    for (const rType of runTypes) {
      const runsForType = runs
        .filter((r) => String(r?.localBackupInfo?.[0]?.runType || "") === rType)
        .sort((a, b) => Number(b?.localBackupInfo?.[0]?.endTimeUsecs || 0) - Number(a?.localBackupInfo?.[0]?.endTimeUsecs || 0));
      if (!runsForType.length) continue;

      // best-effort: id->name mapping for DB host resolution (sourceId)
      const idToName = new Map();
      for (const rr of runsForType) {
        const objs = Array.isArray(rr?.objects) ? rr.objects : [];
        for (const ob of objs) {
          const o = ob?.object;
          if (o?.id && o?.name && !idToName.has(String(o.id))) idToName.set(String(o.id), String(o.name));
        }
      }

      const cleared = new Set();          // object keys cleared by newer success
      const latestFailByKey = new Map();  // first failure seen newest->oldest

      for (const run of runsForType) {
        const info = run?.localBackupInfo?.[0] || {};
        const endTimeET = fmtETFromUsecs(info?.endTimeUsecs);

        const objsAll = (Array.isArray(run?.objects) ? run.objects : []).filter(
          (x) => x?.object && x?.localSnapshotInfo
        );

        // run-level fallback when objects missing
        if (!objsAll.length) {
          if (String(info.status || "") === "Failed") {
            const rk = `RUNLEVEL|${pg.pgId}|${rType}`;
            if (!latestFailByKey.has(rk)) {
              const msg = Array.isArray(info.messages) ? info.messages.join(" | ") : info.messages;
              latestFailByKey.set(rk, {
                Environment: pg.pgEnv,
                Cluster: pg.clusterName,
                ProtectionGroup: pg.pgName,
                Host: "",
                ObjectType: "RunLevel",
                ObjectName: "(Run-level)",
                RunType: rType,
                EndTimeET: endTimeET,
                FailedMessage: cleanMsg(msg),
              });
            }
          }
          continue;
        }

        // 1) mark successes in this run (newer clears older failures)
        for (const ob of objsAll) {
          if (isObjectSuccess(ob)) {
            const k = getObjKey(ob);
            if (k) cleared.add(k);
          }
        }

        // 2) capture latest uncleared failures per object
        for (const ob of objsAll) {
          const k = getObjKey(ob);
          if (!k) continue;
          if (cleared.has(k)) continue;
          if (latestFailByKey.has(k)) continue;
          if (!hasFailedAttempts(ob)) continue;

          const msg = combineFailedAttempts(ob?.localSnapshotInfo?.failedAttempts);
          if (!msg) continue;

          const o = ob.object || {};
          let hostName = "";
          if (o.sourceId && idToName.has(String(o.sourceId))) hostName = idToName.get(String(o.sourceId));

          latestFailByKey.set(k, {
            Environment: o.environment ? String(o.environment) : pg.pgEnv,
            Cluster: pg.clusterName,
            ProtectionGroup: pg.pgName,
            Host: hostName,
            ObjectType: o.objectType ? String(o.objectType) : "UnknownType",
            ObjectName: o.name ? String(o.name) : "",
            RunType: rType,
            EndTimeET: endTimeET,
            FailedMessage: msg,
          });
        }
      }

      for (const v of latestFailByKey.values()) failuresOut.push(v);
    }

    return failuresOut;
  }

  const failures = await mapLimit(pgIndex, CONCURRENCY, processPg);

  return {
    authMode: "vault",
    baseUrl,
    numRuns,
    pgIndexCount: pgIndex.length,
    pgErrors,
    pgErrorsCount: pgErrors.length,
    failures,
    failuresCount: failures.length,
  };
}
