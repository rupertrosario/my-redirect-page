import { credentialVaultClient } from "@dynatrace-sdk/client-classic-environment-v2";
import { result } from "@dynatrace-sdk/automation-utils";

/**
 * PART 3 (HEAVY): Runs logic + failure rule + ONE markdown table
 * READ-ONLY (GET-only)
 *
 * Reads Part 2 output via: await result("run_javascript_2")
 *
 * Failure rule (per runType, per object) within last numRuns:
 * - Latest failure per object that has NO later success (within window)
 *
 * Returns:
 *  - markdownTable
 *  - count
 *  - failures
 */

export default async function () {
  // ---------- pull Part 2 output (NO workflow input mapping required) ----------
  const p2 = await result("run_javascript_2"); // must match your task name exactly
  if (!p2 || !Array.isArray(p2.pgWorkItems)) {
    throw new Error("Part 3: could not read Part 2 output (missing pgWorkItems). Check task name run_javascript_2.");
  }

  const baseUrl = p2.baseUrl || "https://helios.cohesity.com";
  const DAYS_TO_KEEP_MIN = Number(p2.daysToKeepMin ?? 35);
  const NUM_RUNS = Number(p2.numRuns ?? 5);
  const pgWorkItems = p2.pgWorkItems;

  if (!pgWorkItems.length) {
    return {
      markdownTable: `✅ No PGs matched Policy DaysToKeep >= ${DAYS_TO_KEEP_MIN} (nothing to scan).`,
      count: 0,
      failures: [],
    };
  }

  // ---------- AUTH (vault) ----------
  const vaultId = "credentials_vault-312312";
  const d2 = await credentialVaultClient.getCredentialsDetails({ id: vaultId });
  const apiKey = (d2?.token || d2?.password || "").trim();
  if (!apiKey) throw new Error("No Helios API key available (empty token/password).");

  const commonHeaders = { accept: "application/json", apiKey };

  // ---------- fetch helper ----------
  async function getJson(url, headers) {
    const resp = await fetch(url, { method: "GET", headers });
    if (!resp.ok) {
      let txt = "";
      try { txt = await resp.text(); } catch (_) {}
      throw new Error(`GET ${url} -> HTTP ${resp.status} ${txt}`);
    }
    return resp.json();
  }

  // ---------- utils ----------
  function cleanMsg(s) {
    if (!s) return "";
    return String(s)
      .replace(/[\r\n]+/g, " ")
      .replace(/,/g, " ")
      .replace(/"/g, "'")
      .trim();
  }

  function fmtETFromUsecs(usecs) {
    if (!usecs) return "";
    const ms = Math.floor(Number(usecs) / 1000);
    const d = new Date(ms);
    try {
      return new Intl.DateTimeFormat("en-US", {
        timeZone: "America/New_York",
        year: "numeric",
        month: "2-digit",
        day: "2-digit",
        hour: "2-digit",
        minute: "2-digit",
        hour12: false,
      }).format(d);
    } catch {
      return d.toISOString();
    }
  }

  function getObjKey(ob) {
    const o = ob?.object;
    if (!o) return "";
    if (o.id) return String(o.id);
    const sid = o.sourceId ? String(o.sourceId) : "";
    return `${o.environment || ""}|${o.objectType || ""}|${o.name || ""}|${sid}`;
  }

  function hasFailedAttempts(ob) {
    const fa = ob?.localSnapshotInfo?.failedAttempts;
    return Array.isArray(fa) && fa.length > 0;
  }

  function isSuccessForClear(ob) {
    return ob?.localSnapshotInfo && !hasFailedAttempts(ob);
  }

  function combineFailedAttempts(attempts) {
    if (!Array.isArray(attempts) || attempts.length === 0) return "";
    const msgs = attempts.map((a) => cleanMsg(a?.message)).filter(Boolean);
    return msgs.join(" | ");
  }

  // ---------- group PGs by cluster (so headers are correct + optional parallelism) ----------
  const byCluster = new Map(); // clusterId -> { clusterName, items: [...] }
  for (const it of pgWorkItems) {
    const cid = String(it?.clusterId || "");
    if (!cid) continue;
    if (!byCluster.has(cid)) byCluster.set(cid, { clusterName: it?.clusterName || `Unknown-${cid}`, items: [] });
    byCluster.get(cid).items.push(it);
  }

  // ---------- small concurrency pool (keeps runtime down without hammering Helios) ----------
  async function mapLimit(arr, limit, fn) {
    const out = [];
    let i = 0;
    const workers = Array.from({ length: Math.min(limit, arr.length) }, async () => {
      while (i < arr.length) {
        const idx = i++;
        try { out[idx] = await fn(arr[idx], idx); }
        catch (e) { out[idx] = { __error: e }; }
      }
    });
    await Promise.all(workers);
    return out;
  }

  const CONCURRENCY_PER_CLUSTER = 3;
  const allFailures = [];

  for (const [clusterId, group] of byCluster.entries()) {
    const headers = {
      ...commonHeaders,
      Accept: "application/json",
      accessClusterId: String(clusterId),
    };

    async function processPg(pgItem) {
      const pgId = String(pgItem.pgId || "");
      const pgName = pgItem.pgName || "";
      const pgEnv = pgItem.pgEnv || "UnknownEnv";
      const clusterName = pgItem.clusterName || group.clusterName;

      if (!pgId) return;

      // 1) LIGHT runs
      let runsLight;
      try {
        runsLight = await getJson(
          `${baseUrl}/v2/data-protect/protection-groups/${encodeURIComponent(pgId)}/runs?numRuns=${NUM_RUNS}&excludeNonRestorableRuns=false&includeObjectDetails=false`,
          headers
        );
      } catch (_) {
        return; // fail-open per PG
      }

      const runsArr = Array.isArray(runsLight?.runs) ? runsLight.runs : [];
      if (!runsArr.length) return;

      const runTypes = Array.from(
        new Set(
          runsArr
            .map((r) => r?.localBackupInfo?.[0]?.runType)
            .filter(Boolean)
            .map(String)
        )
      );

      let needHeavy = false;
      for (const rt of runTypes) {
        const latest = runsArr
          .filter((r) => String(r?.localBackupInfo?.[0]?.runType || "") === rt)
          .sort(
            (a, b) => Number(b?.localBackupInfo?.[0]?.endTimeUsecs || 0) - Number(a?.localBackupInfo?.[0]?.endTimeUsecs || 0)
          )[0];
        const st = latest?.localBackupInfo?.[0]?.status;
        if (st !== "Succeeded") { needHeavy = true; break; }
      }
      if (!needHeavy) return;

      // 2) HEAVY runs
      let runsHeavy;
      try {
        runsHeavy = await getJson(
          `${baseUrl}/v2/data-protect/protection-groups/${encodeURIComponent(pgId)}/runs?numRuns=${NUM_RUNS}&excludeNonRestorableRuns=false&includeObjectDetails=true`,
          headers
        );
      } catch (_) {
        return;
      }

      const runs = Array.isArray(runsHeavy?.runs) ? runsHeavy.runs : [];
      if (!runs.length) return;

      const heavyRunTypes = Array.from(
        new Set(
          runs
            .map((r) => r?.localBackupInfo?.[0]?.runType)
            .filter(Boolean)
            .map(String)
        )
      );

      for (const rType of heavyRunTypes) {
        const runsForType = runs
          .filter((r) => String(r?.localBackupInfo?.[0]?.runType || "") === rType)
          .sort(
            (a, b) => Number(b?.localBackupInfo?.[0]?.endTimeUsecs || 0) - Number(a?.localBackupInfo?.[0]?.endTimeUsecs || 0)
          );
        if (!runsForType.length) continue;

        // DB host map (best-effort)
        const idToName = new Map();
        for (const rr of runsForType) {
          const objs = Array.isArray(rr?.objects) ? rr.objects : [];
          for (const ob of objs) {
            const o = ob?.object;
            if (o?.id && o?.name && !idToName.has(String(o.id))) idToName.set(String(o.id), String(o.name));
          }
        }

        const cleared = new Set();
        const latestFailByKey = new Map();

        for (const run of runsForType) {
          const info = run?.localBackupInfo?.[0] || {};
          const endTimeET = fmtETFromUsecs(info.endTimeUsecs);

          const objsAll = (Array.isArray(run?.objects) ? run.objects : [])
            .filter((x) => x?.object && x?.localSnapshotInfo);

          // Run-level fallback only if objects missing and run Failed
          if (!objsAll.length) {
            if (info.status === "Failed") {
              const rk = `RUNLEVEL|${pgId}|${rType}`;
              if (!latestFailByKey.has(rk)) {
                const msg = Array.isArray(info.messages) ? info.messages.join(" | ") : info.messages;
                latestFailByKey.set(rk, {
                  Environment: pgEnv,
                  Cluster: clusterName,
                  ProtectionGroup: pgName,
                  Host: "",
                  ObjectType: "RunLevel",
                  ObjectName: "(Run-level)",
                  RunType: rType,
                  EndTimeET: endTimeET,
                  FailedMessage: cleanMsg(msg),
                });
              }
            }
            continue;
          }

          // Pass 1: clear successes (newer clears older failures)
          for (const ob of objsAll) {
            if (isSuccessForClear(ob)) {
              const k = getObjKey(ob);
              if (k) cleared.add(k);
            }
          }

          // Pass 2: capture latest uncleared failures
          for (const ob of objsAll) {
            const k = getObjKey(ob);
            if (!k) continue;
            if (cleared.has(k)) continue;
            if (latestFailByKey.has(k)) continue;
            if (!hasFailedAttempts(ob)) continue;

            const msg = combineFailedAttempts(ob?.localSnapshotInfo?.failedAttempts);
            if (!msg) continue;

            const o = ob.object || {};
            let hostName = "";
            if (o.sourceId && idToName.has(String(o.sourceId))) hostName = idToName.get(String(o.sourceId));

            latestFailByKey.set(k, {
              Environment: o.environment ? String(o.environment) : pgEnv,
              Cluster: clusterName,
              ProtectionGroup: pgName,
              Host: hostName,
              ObjectType: o.objectType ? String(o.objectType) : "UnknownType",
              ObjectName: o.name ? String(o.name) : "",
              RunType: rType,
              EndTimeET: endTimeET,
              FailedMessage: msg,
            });
          }
        }

        for (const v of latestFailByKey.values()) allFailures.push(v);
      }
    }

    // process PGs with small concurrency per cluster
    await mapLimit(group.items, CONCURRENCY_PER_CLUSTER, processPg);
  }

  // ---------- output ----------
  if (!allFailures.length) {
    return {
      markdownTable: `✅ No failures found for Policy DaysToKeep >= ${DAYS_TO_KEEP_MIN} across all clusters (last ${NUM_RUNS} runs).`,
      count: 0,
      failures: [],
    };
  }

  // final dedup safety
  const dedup = new Map();
  for (const r of allFailures) {
    const key = `${r.Environment}|${r.Cluster}|${r.ProtectionGroup}|${r.RunType}|${r.Host}|${r.ObjectType}|${r.ObjectName}`;
    const prev = dedup.get(key);
    if (!prev || String(r.EndTimeET) > String(prev.EndTimeET)) dedup.set(key, r);
  }

  const final = Array.from(dedup.values()).sort((a, b) => {
    const c = String(a.Cluster).localeCompare(String(b.Cluster));
    if (c) return c;
    const p = String(a.ProtectionGroup).localeCompare(String(b.ProtectionGroup));
    if (p) return p;
    const e = String(a.Environment).localeCompare(String(b.Environment));
    if (e) return e;
    const rt = String(a.RunType).localeCompare(String(b.RunType));
    if (rt) return rt;
    return String(b.EndTimeET).localeCompare(String(a.EndTimeET));
  });

  const cols = ["Environment", "Cluster", "ProtectionGroup", "Host", "ObjectType", "ObjectName", "RunType", "EndTimeET", "FailedMessage"];
  const lines = [];
  lines.push(`### Cohesity PROD Failures (Policy DaysToKeep >= ${DAYS_TO_KEEP_MIN}) — All Clusters`);
  lines.push(`Window: last **${NUM_RUNS}** runs per runType`);
  lines.push(`Total failures: **${final.length}**`);
  lines.push("");
  lines.push(`| ${cols.join(" | ")} |`);
  lines.push(`| ${cols.map(() => "---").join(" | ")} |`);

  for (const r of final) {
    const row = [
      r.Environment,
      r.Cluster,
      r.ProtectionGroup,
      r.Host || "",
      r.ObjectType,
      r.ObjectName,
      r.RunType,
      r.EndTimeET,
      cleanMsg(r.FailedMessage),
    ].map(cleanMsg);
    lines.push(`| ${row.join(" | ")} |`);
  }

  return {
    markdownTable: lines.join("\n"),
    count: final.length,
    failures: final,
  };
}
