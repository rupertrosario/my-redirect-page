import { credentialVaultClient } from "@dynatrace-sdk/client-classic-environment-v2";

/**
 * Cohesity PROD Failures (Policy DaysToKeep >= 35) — Multi-Cluster (Helios)
 * STRICTLY READ-ONLY (GET-only)
 *
 * NO selection menus:
 * - Runs across ALL clusters
 * - Includes ALL active PGs (isDeleted=false, isPaused=false, isActive=true)
 *
 * Logic (minimal-change, correct partial-rerun handling):
 * - Pre-filter PGs by Policy daysToKeep >= 35 (policy calls cached per cluster)
 * - For each PG: fetch last N runs (default 20) includeObjectDetails=true
 * - Cross-RunType clearing:
 *     object success (failedAttempts empty) clears older failures for that object
 * - Capture latest uncleared failure per object (first failure seen newest->oldest)
 *
 * Output:
 * - markdownEmail (single consolidated markdown table)
 * - failures[] + count
 *
 * Vaulting: EXACT style (vaultId + token/password)
 */

export default async function () {
  const baseUrl = "https://helios.cohesity.com";
  const NUM_RUNS = Number(process.env.NUM_RUNS || 20);
  const MIN_DAYS_TO_KEEP = Number(process.env.MIN_DAYS_TO_KEEP || 35);

  // ==============================
  // AUTH (vault id) ✅ EXACT style
  // ==============================
  const vaultId = "credentials_vault-312312";
  const d2 = await credentialVaultClient.getCredentialsDetails({ id: vaultId });
  const apiKey = (d2?.token || d2?.password || "").trim();
  if (!apiKey) throw new Error("No Helios API key available (empty token/password).");

  const commonHeaders = { accept: "application/json", apiKey };

  // ==============================
  // GET helper
  // ==============================
  async function getJson(url, headers) {
    const resp = await fetch(url, { method: "GET", headers });
    if (!resp.ok) {
      let txt = "";
      try { txt = await resp.text(); } catch (_) {}
      throw new Error(`GET ${url} -> HTTP ${resp.status} ${txt}`);
    }
    return resp.json();
  }

  // ==============================
  // Helpers
  // ==============================
  const sleep = (ms) => new Promise((r) => setTimeout(r, ms));

  async function getJsonWithRetry(url, headers, retries = 2) {
    for (let i = 0; i <= retries; i++) {
      const resp = await fetch(url, { method: "GET", headers });
      if (resp.ok) return resp.json();

      const st = resp.status;
      if ((st === 429 || st >= 500) && i < retries) {
        await sleep(700 * (i + 1));
        continue;
      }

      let txt = "";
      try { txt = await resp.text(); } catch (_) {}
      throw new Error(`GET ${url} -> HTTP ${st} ${txt}`);
    }
    return null;
  }

  function toArray(v) {
    if (v == null) return [];
    return Array.isArray(v) ? v : [v];
  }

  function cleanMsg(s) {
    if (!s) return "";
    return String(s)
      .replace(/[\r\n]+/g, " ")
      .replace(/\|/g, " ")
      .replace(/"/g, "'")
      .replace(/,/g, " ")
      .trim();
  }

  function fmtETFromUsecs(usecs) {
    if (!usecs) return "";
    const ms = Math.floor(Number(usecs) / 1000);
    const dt = new Date(ms);
    try {
      return new Intl.DateTimeFormat("en-US", {
        timeZone: "America/New_York",
        year: "numeric",
        month: "2-digit",
        day: "2-digit",
        hour: "2-digit",
        minute: "2-digit",
        hour12: false,
      }).format(dt);
    } catch {
      return dt.toISOString();
    }
  }

  function getInfo(run) {
    const lb = run?.localBackupInfo;
    if (!lb) return null;
    return Array.isArray(lb) ? (lb[0] ?? null) : lb; // normalize singleton
  }

  function getObjKey(ob) {
    const o = ob?.object;
    if (!o) return "";
    // stable-ish key (matches your PS idea): env|type|name|sourceId
    const sid = o.sourceId ? String(o.sourceId) : "";
    return `${o.environment || ""}|${o.objectType || ""}|${o.name || ""}|${sid}`;
  }

  function failedAttemptsArr(ob) {
    return toArray(ob?.localSnapshotInfo?.failedAttempts); // normalize singleton
  }

  function hasFailedAttempts(ob) {
    return failedAttemptsArr(ob).length > 0;
  }

  function isObjectSuccess(ob) {
    return !!ob?.localSnapshotInfo && !hasFailedAttempts(ob);
  }

  function combineFailedAttempts(attempts) {
    const arr = toArray(attempts);
    const msgs = arr.map((a) => cleanMsg(a?.message)).filter(Boolean);
    return msgs.join(" | ");
  }

  async function mapLimit(items, limit, fn) {
    const out = [];
    let i = 0;
    const workers = Array.from({ length: Math.min(limit, items.length) }, async () => {
      while (true) {
        const idx = i++;
        if (idx >= items.length) break;
        const vals = await fn(items[idx], idx);
        if (Array.isArray(vals) && vals.length) out.push(...vals);
      }
    });
    await Promise.all(workers);
    return out;
  }

  function mdEsc(s) {
    return String(s ?? "").replace(/\|/g, "\\|").replace(/[\r\n]+/g, " ").trim();
  }

  // ==============================
  // 1) CLUSTERS (ALL)
  // ==============================
  const clu = await getJson(`${baseUrl}/v2/mcm/cluster-mgmt/info`, { ...commonHeaders, Accept: "application/json" });
  const clusters = Array.isArray(clu?.cohesityClusters) ? clu.cohesityClusters : [];
  if (!clusters.length) throw new Error("No clusters returned from /v2/mcm/cluster-mgmt/info");

  // ==============================
  // 2) Build PG index across ALL clusters (active only)
  // ==============================
  const pgIndex = [];

  for (const cl of clusters) {
    const clusterId = cl?.clusterId;
    const clusterName =
      String(cl?.name || cl?.clusterName || cl?.displayName || "").trim() || `Unknown-${clusterId}`;
    if (!clusterId) continue;

    const headers = { ...commonHeaders, Accept: "application/json", accessClusterId: String(clusterId) };

    let pgJson;
    try {
      pgJson = await getJson(
        `${baseUrl}/v2/data-protect/protection-groups?isDeleted=false&isPaused=false&isActive=true`,
        headers
      );
    } catch (_) {
      continue; // fail-open per cluster
    }

    const pgs = Array.isArray(pgJson?.protectionGroups) ? pgJson.protectionGroups : [];
    for (const pg of pgs) {
      pgIndex.push({
        clusterId: String(clusterId),
        clusterName,
        pgId: String(pg?.id ?? "").trim(),
        pgName: String(pg?.name ?? "").trim(),
        policyId: String(pg?.policyId ?? "").trim(),
        pgEnv: pg?.environment ?? "",
      });
    }
  }

  if (!pgIndex.length) {
    return {
      markdownEmail: `✅ No active Protection Groups found across all clusters.`,
      failures: [],
      count: 0,
    };
  }

  // ==============================
  // 3) Policy cache per cluster + filter PGs by daysToKeep >= MIN_DAYS_TO_KEEP
  // ==============================
  const policyCacheByCluster = new Map(); // clusterId -> Map(policyId -> {daysToKeep,name})

  async function getPolicyCached(clusterId, headers, policyId) {
    if (!policyId) return null;
    if (!policyCacheByCluster.has(clusterId)) policyCacheByCluster.set(clusterId, new Map());
    const cache = policyCacheByCluster.get(clusterId);
    if (cache.has(policyId)) return cache.get(policyId);

    const enc = encodeURIComponent(String(policyId));
    const url = `${baseUrl}/irisservices/api/v1/public/protectionPolicies/${enc}`;
    try {
      const p = await getJson(url, headers);
      const obj = { daysToKeep: p?.daysToKeep ?? null, name: p?.name ?? "" };
      cache.set(policyId, obj);
      return obj;
    } catch (_) {
      const obj = { daysToKeep: null, name: "PolicyLookupFailed" };
      cache.set(policyId, obj);
      return obj;
    }
  }

  const pgIndex35 = [];
  for (const pg of pgIndex) {
    if (!pg.policyId) continue;
    const headers = { ...commonHeaders, Accept: "application/json", accessClusterId: String(pg.clusterId) };
    const pol = await getPolicyCached(String(pg.clusterId), headers, String(pg.policyId));
    const days = pol?.daysToKeep;
    if (days !== null && Number(days) >= MIN_DAYS_TO_KEEP) pgIndex35.push(pg);
  }

  if (!pgIndex35.length) {
    return {
      markdownEmail: `✅ No active PGs meet Policy daysToKeep >= ${MIN_DAYS_TO_KEEP} across all clusters.`,
      failures: [],
      count: 0,
    };
  }

  // ==============================
  // 4) Runs + failure logic (heavy)
  // ==============================
  const CONCURRENCY = Number(process.env.CONCURRENCY || 3);
  const pgErrors = [];

  async function processPg(pg) {
    const headers = { ...commonHeaders, Accept: "application/json", accessClusterId: String(pg.clusterId) };
    const url =
      `${baseUrl}/v2/data-protect/protection-groups/${encodeURIComponent(String(pg.pgId))}` +
      `/runs?numRuns=${NUM_RUNS}&excludeNonRestorableRuns=false&includeObjectDetails=true`;

    let jsonRuns;
    try {
      jsonRuns = await getJsonWithRetry(url, headers, 2);
    } catch (e) {
      pgErrors.push({ cluster: pg.clusterName, pg: pg.pgName, error: String(e?.message || e) });
      return [];
    }

    const runs = toArray(jsonRuns?.runs);
    if (!runs.length) return [];

    const runTypes = Array.from(
      new Set(
        runs
          .map((r) => getInfo(r)?.runType)
          .filter(Boolean)
          .map((x) => String(x))
      )
    );

    const failuresOut = [];

    for (const rType of runTypes) {
      const runsForType = runs
        .filter((r) => String(getInfo(r)?.runType || "") === rType)
        .sort((a, b) => Number(getInfo(b)?.endTimeUsecs || 0) - Number(getInfo(a)?.endTimeUsecs || 0));
      if (!runsForType.length) continue;

      const idToName = new Map();
      for (const rr of runsForType) {
        for (const ob of toArray(rr?.objects)) {
          const o = ob?.object;
          if (o?.id && o?.name && !idToName.has(String(o.id))) idToName.set(String(o.id), String(o.name));
        }
      }

      const cleared = new Set();
      const latestFailByKey = new Map();

      for (const run of runsForType) {
        const info = getInfo(run) || {};
        const endTimeET = fmtETFromUsecs(info?.endTimeUsecs);

        const objsAll = toArray(run?.objects).filter((x) => x?.object && x?.localSnapshotInfo);

        if (!objsAll.length) {
          if (String(info.status || "") === "Failed") {
            const rk = `RUNLEVEL|${pg.pgId}|${rType}`;
            if (!latestFailByKey.has(rk)) {
              const msg = Array.isArray(info.messages) ? info.messages.join(" | ") : info.messages;
              latestFailByKey.set(rk, {
                Environment: pg.pgEnv || "UnknownEnv",
                Cluster: pg.clusterName,
                ProtectionGroup: pg.pgName,
                Host: "",
                ObjectType: "RunLevel",
                ObjectName: "(Run-level)",
                RunType: rType,
                EndTimeET: endTimeET,
                FailedMessage: cleanMsg(msg),
              });
            }
          }
          continue;
        }

        // cross-runType clearing
        for (const ob of objsAll) {
          if (isObjectSuccess(ob)) {
            const k = getObjKey(ob);
            if (k) cleared.add(k);
          }
        }

        // latest uncleared failure
        for (const ob of objsAll) {
          const k = getObjKey(ob);
          if (!k) continue;
          if (cleared.has(k)) continue;
          if (latestFailByKey.has(k)) continue;
          if (!hasFailedAttempts(ob)) continue;

          const msg = combineFailedAttempts(failedAttemptsArr(ob));
          if (!msg) continue;

          const o = ob.object || {};
          let hostName = "";
          if (o.sourceId && idToName.has(String(o.sourceId))) hostName = idToName.get(String(o.sourceId));

          latestFailByKey.set(k, {
            Environment: o.environment ? String(o.environment) : (pg.pgEnv || "UnknownEnv"),
            Cluster: pg.clusterName,
            ProtectionGroup: pg.pgName,
            Host: hostName,
            ObjectType: o.objectType ? String(o.objectType) : "UnknownType",
            ObjectName: o.name ? String(o.name) : "",
            RunType: rType,
            EndTimeET: endTimeET,
            FailedMessage: msg,
          });
        }
      }

      for (const v of latestFailByKey.values()) failuresOut.push(v);
    }

    return failuresOut;
  }

  const allFailures = await mapLimit(pgIndex35, CONCURRENCY, processPg);

  // final dedup safety (same idea as your reference)
  const dedup = new Map();
  for (const r of allFailures) {
    const key = `${r.Environment}|${r.Cluster}|${r.ProtectionGroup}|${r.RunType}|${r.Host}|${r.ObjectType}|${r.ObjectName}`;
    const prev = dedup.get(key);
    if (!prev || String(r.EndTimeET) > String(prev.EndTimeET)) dedup.set(key, r);
  }

  const final = Array.from(dedup.values()).sort((a, b) => {
    const c = String(a.Cluster).localeCompare(String(b.Cluster));
    if (c) return c;
    const p = String(a.ProtectionGroup).localeCompare(String(b.ProtectionGroup));
    if (p) return p;
    const e = String(a.Environment).localeCompare(String(b.Environment));
    if (e) return e;
    const rt = String(a.RunType).localeCompare(String(b.RunType));
    if (rt) return rt;
    return String(b.EndTimeET).localeCompare(String(a.EndTimeET));
  });

  // markdownEmail (single body)
  if (!final.length) {
    return {
      markdownEmail: `✅ No failures found for Policy DaysToKeep >= ${MIN_DAYS_TO_KEEP} across ALL clusters (Last ${NUM_RUNS} runs).`,
      failures: [],
      count: 0,
      pgErrors,
      pgErrorsCount: pgErrors.length,
    };
  }

  const headers = [
    "Environment",
    "Cluster",
    "ProtectionGroup",
    "Host",
    "ObjectType",
    "ObjectName",
    "RunType",
    "EndTimeET",
    "FailedMessage",
  ];

  const lines = [];
  lines.push(`### Cohesity PROD Failures (Policy DaysToKeep >= ${MIN_DAYS_TO_KEEP}) — ALL Clusters`);
  lines.push(`PGs scanned (>=${MIN_DAYS_TO_KEEP}d): **${pgIndex35.length}** | Failures: **${final.length}** | Runs: **${NUM_RUNS}**`);
  if (pgErrors.length) lines.push(`PG fetch errors: **${pgErrors.length}** (see pgErrors[])`);
  lines.push("");
  lines.push(`| ${headers.join(" | ")} |`);
  lines.push(`| ${headers.map(() => "---").join(" | ")} |`);
  for (const r of final) {
    const row = [
      r.Environment,
      r.Cluster,
      r.ProtectionGroup,
      r.Host || "",
      r.ObjectType,
      r.ObjectName,
      r.RunType,
      r.EndTimeET,
      cleanMsg(r.FailedMessage),
    ].map(mdEsc);
    lines.push(`| ${row.join(" | ")} |`);
  }

  return {
    markdownEmail: lines.join("\n"),
    failures: final,
    count: final.length,
    pgErrors,
    pgErrorsCount: pgErrors.length,
    scanned: { clusters: clusters.length, pgsActive: pgIndex.length, pgs35: pgIndex35.length },
    tunables: { NUM_RUNS, MIN_DAYS_TO_KEEP, CONCURRENCY },
  };
}
